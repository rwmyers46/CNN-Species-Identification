{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Download & Processing Functions:\n",
    "\n",
    "The functions below downloads images specified by query from Microsoft Cognitive Services API and processes the files to remove corrupted & duplicate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import exceptions\n",
    "import argparse\n",
    "import requests\n",
    "import cv2\n",
    "import os\n",
    "import itertools as it\n",
    "import shutil\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to image download directory:\n",
    "\n",
    "path = '/path-to-image-directory/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download from Microsoft Cognitive Services API:\n",
    "Note: download code adapted from Adrian Rosebrock post: https://www.pyimagesearch.com/2018/04/09/how-to-quickly-build-a-deep-learning-image-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDownloadProcess():\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        '''\n",
    "        Initiate class with vars and inputs\n",
    "        '''\n",
    "        self.API_KEY = \"867-5309\"\n",
    "        self.output_path = path\n",
    "        self.URL = \"https://api.cognitive.microsoft.com/bing/v7.0/images/search\"\n",
    "        self.GROUP_SIZE = 50\n",
    "        self.MAX_RESULTS = int(input('Total images to download:'))\n",
    "        self.image_offset = int(input('Specify offset:'))\n",
    "        self.name_size = int(input('Enter Name_Size:'))\n",
    "\n",
    "    def run(self):\n",
    "        '''\n",
    "        Run download API\n",
    "        '''\n",
    "        # build exceptions filter\n",
    "        EXCEPTIONS = set([IOError, FileNotFoundError, exceptions.RequestException, exceptions.HTTPError,\n",
    "                          exceptions.ConnectionError, exceptions.Timeout])\n",
    "\n",
    "        # store the search term and set headers and search parameters:\n",
    "        term = input('Search term:')\n",
    "        headers = {\"Ocp-Apim-Subscription-Key\" : self.API_KEY}\n",
    "        params = {\"q\": term, \"offset\": self.image_offset, \"count\": self.GROUP_SIZE}\n",
    "\n",
    "        # initiate search:\n",
    "        print(\"[INFO] searching Bing API for '{}'\".format(term))\n",
    "        search = requests.get(self.URL, headers=headers, params=params)\n",
    "        search.raise_for_status()\n",
    "\n",
    "        # estimate results returned by the Bing API:\n",
    "        results = search.json()\n",
    "        estNumResults = min(results[\"totalEstimatedMatches\"], self.MAX_RESULTS)\n",
    "        print(\"[INFO] {} total results for '{}'\".format(estNumResults,\n",
    "            term))\n",
    "\n",
    "        total = 0\n",
    "\n",
    "        # loop over the estimated number of results in `GROUP_SIZE` groups\n",
    "        for offset in range(0, estNumResults, self.GROUP_SIZE):\n",
    "            # update the search parameters using the current offset, then\n",
    "            # make the request to fetch the results\n",
    "            print(\"[INFO] making request for group {}-{} of {}...\".format(\n",
    "                offset, offset + self.GROUP_SIZE, estNumResults))\n",
    "            params[\"offset\"] = offset\n",
    "            search = requests.get(self.URL, headers=headers, params=params)\n",
    "            search.raise_for_status()\n",
    "            results = search.json()\n",
    "            print(\"[INFO] saving images for group {}-{} of {}...\".format(\n",
    "                offset, offset + self.GROUP_SIZE, estNumResults))\n",
    "\n",
    "            ## loop over the results:\n",
    "            for v in results[\"value\"]:\n",
    "\n",
    "                try:\n",
    "                    # make a request to download the image\n",
    "                    print(\"[INFO] fetching: {}\".format(v[\"contentUrl\"]))\n",
    "                    r = requests.get(v[\"contentUrl\"], timeout=30)\n",
    "\n",
    "                    # build the path to the output image\n",
    "                    ext = v[\"contentUrl\"][v[\"contentUrl\"].rfind(\".\"):]\n",
    "                    p = os.path.sep.join([self.output_path, \"{}{}\".format(\n",
    "                        str(total).zfill(8), ext)])\n",
    "\n",
    "                    # write the image to disk\n",
    "                    f = open(p, \"wb\")\n",
    "                    f.write(r.content)\n",
    "                    f.close()\n",
    "\n",
    "                # catch errors that may cause download to abort:\n",
    "                except Exception as e:\n",
    "                    if type(e) in EXCEPTIONS:\n",
    "                        print(\"[INFO] skipping: {}\".format(v[\"contentUrl\"]))\n",
    "                        continue\n",
    "                image = cv2.imread(p)\n",
    "\n",
    "                # if the image is `None` then we remove it:\n",
    "                if image is None:\n",
    "                    print(\"[INFO] deleting: {}\".format(p))\n",
    "                    os.remove(p)\n",
    "                    continue\n",
    "\n",
    "                total += 1\n",
    "        print(\"\\n Download Complete\")\n",
    "        \n",
    "        ## call image processing methods:\n",
    "        self.bad_pics()\n",
    "        self.check_duplicates()\n",
    "        self.name_class()\n",
    "        \n",
    "\n",
    "    def bad_pics(self):\n",
    "        '''\n",
    "        Process image files in directory_path to detect corrupted or None types:\n",
    "        '''\n",
    "        bad_pics = []\n",
    "        i = 0\n",
    "\n",
    "        for fname in os.listdir(self.path):\n",
    "            try:\n",
    "                img = Image.open(self.path + fname) # open image file\n",
    "                img.verify() # verify file is an image\n",
    "                cv2.imread(fname)\n",
    "            except:\n",
    "                bad_pics.append(fname)\n",
    "            i += 1\n",
    "        print(\"total files:\", i)\n",
    "        print(\"bad files:\", len(bad_pics))\n",
    "\n",
    "        if len(bad_pics) != 0:\n",
    "            user_answer = input('Remove bad files? [y] or [n]')\n",
    "\n",
    "            if user_answer == 'y':\n",
    "                ctr = 0\n",
    "                for bp in bad_pics:\n",
    "                    try:\n",
    "                        print(\"Deleting... {}\".format(bp))\n",
    "                        os.remove(self.path + bp)\n",
    "                    except:\n",
    "                        if bp == '.ipynb_checkpoints':\n",
    "                            shutil.rmtree(self.path + bp)     # remove checkpoint file if present\n",
    "                    ctr += 1\n",
    "                print('Deleted {} files.'.format(ctr))\n",
    "            else:\n",
    "                print('No files deleted.')\n",
    "\n",
    "\n",
    "    def image_optimizer(self):\n",
    "        '''\n",
    "        Group images by size to increase computational efficiency\n",
    "        '''\n",
    "\n",
    "        short_list = []\n",
    "        files_investigate = {}\n",
    "        counts = dict()\n",
    "\n",
    "        ## find unique image size values & store in a dictionary:\n",
    "        size_list = [n[1] for m,n in enumerate(self.name_size)]  \n",
    "\n",
    "        poss_dupes = set(size_list)     \n",
    "\n",
    "        for m, n in enumerate(name_size):\n",
    "            if n[1] in poss_dupes:           \n",
    "                counts[n[1]] = counts.get(n[1], 0) + 1\n",
    "\n",
    "        ## remove items with values = 1 (no need to process images of unique sizes) to use as a checksum:\n",
    "        short_list = [c for c in counts if counts[c] > 1]  \n",
    "\n",
    "        ## create a dictionary where keys = file size and values = lists of file names.\n",
    "        ## this reduces processing by ensuring images are only compared to others within groups of identical sizes\n",
    "        for ns in name_size:\n",
    "            if ns[1] in files_investigate:\n",
    "                 files_investigate[ns[1]].append(ns[0])\n",
    "            else:\n",
    "                files_investigate[ns[1]] = [ns[0]]\n",
    "\n",
    "        ## dictionary filters `files_investigate` for values > 1:\n",
    "        filtered_dict = defaultdict(list)\n",
    "\n",
    "        for k, v in files_investigate.items():\n",
    "            if len(v) > 1:\n",
    "                filtered_dict[k].append(v)\n",
    "\n",
    "        if len(short_list) == len(filtered_dict):\n",
    "            print('>> Created {} groups of images to compare\\n'.format(len(short_list)))\n",
    "            return(filtered_dict)\n",
    "        else:\n",
    "            print(\"Error detected.\")\n",
    "            \n",
    "\n",
    "    def check_duplicates(self):\n",
    "        '''\n",
    "        Check for redundant images in <path> directory:\n",
    "        '''\n",
    "        \n",
    "        duplicates_list, corrupted_list, size_list, name_size = [], [], [], []\n",
    "        comp_dict = {}\n",
    "\n",
    "        # create list of all files in directory & check for errors:\n",
    "        img_list = [i for i in os.listdir(self.path)] \n",
    "\n",
    "        for i, j in enumerate(img_list):\n",
    "            read = cv2.imread(self.path + j)\n",
    "\n",
    "            # create list of tuples in filename, size format:\n",
    "            try:\n",
    "                temp = read.shape\n",
    "                size_list = (j, temp)\n",
    "                name_size.append(size_list)\n",
    "\n",
    "            except:\n",
    "                if j == '.ipynb_checkpoints':\n",
    "                    shutil.rmtree(self.path + j)     # remove jupyter labs checkpoint file if present\n",
    "                else:\n",
    "                    print('Bad image found:', j)\n",
    "\n",
    "        print('Original image list size:', len(name_size))\n",
    "\n",
    "        # optimize processing by organizing images into groups of equal size:\n",
    "        prepped_images = Image_Optimizer(name_size)   \n",
    "\n",
    "        # conduct pairwise comparison of images sharing a key:\n",
    "        for k, v in prepped_images.items():\n",
    "            v = sum(v, [])                        # flatten values list\n",
    "            img_combos = it.combinations(v, 2)    # create list of pairwise combinations from values\n",
    "\n",
    "            print('Image combinations being processed:', len(list(img_combos)))\n",
    "\n",
    "            for i, j in enumerate(img_combos):\n",
    "                try:\n",
    "                    original = cv2.imread(self.path + j[0])\n",
    "                    duplicate = cv2.imread(self.path + j[1])\n",
    "\n",
    "                    if original.shape == duplicate.shape:    # double check that image dimensions equal \n",
    "\n",
    "                    # compute image differences and split by channel:\n",
    "                        difference = cv2.subtract(original, duplicate)\n",
    "                        b, g, r = cv2.split(difference)\n",
    "\n",
    "                        if cv2.countNonZero(b) == 0 and cv2.countNonZero(g) == 0 and cv2.countNonZero(r) == 0:\n",
    "                            print('Images are completely Equal:', j)\n",
    "                            duplicates_list.append(j)    # append duplicate filenames to list\n",
    "\n",
    "                except:\n",
    "                    print('Bad file(s) detected:', fname)\n",
    "                    corrupted_list.append(fname)\n",
    "\n",
    "        print('Duplicates: ', len(duplicates_list))\n",
    "        print('Corrupted: ', len(corrupted_list))\n",
    "\n",
    "        if len(duplicates_list) > 0:\n",
    "\n",
    "            d_list = [i[0] for i in duplicates_list]\n",
    "\n",
    "            for dl in duplicates_list:\n",
    "                if dl[0] in comp_dict:\n",
    "                    comp_dict[dl[0]].append(dl[1])\n",
    "                else:\n",
    "                    comp_dict[dl[0]] = [dl[1]]\n",
    "\n",
    "            # compile a list of values from comp_dict to delete:\n",
    "            dump_list = []\n",
    "\n",
    "            for cd, v in comp_dict.items():\n",
    "                dump_list.extend(v)\n",
    "\n",
    "            print('Dump List:',len(dump_list))\n",
    "\n",
    "            # remove duplicate values and convert to ordered data structure\n",
    "            dump_list = list(set(dump_list))    \n",
    "            print(dump_list)\n",
    "\n",
    "            # user confirmation to delete files:\n",
    "            user_answer = input('Remove duplicate files? [y] or [n]')\n",
    "\n",
    "            if user_answer == 'y':\n",
    "                ctr = 0\n",
    "                for d in dump_list:\n",
    "                    os.remove(self.path + d)\n",
    "                    ctr += 1\n",
    "                print('Deleted {} files.'.format(ctr))\n",
    "                \n",
    "\n",
    "    def name_class(self):\n",
    "        '''\n",
    "        Rename files to be recognized by Keras - be sure to include trailing / in path\n",
    "        '''\n",
    "        \n",
    "        backup_list = []\n",
    "\n",
    "        print('Found {} files in directory.'.format(len(os.listdir(self.path))))\n",
    "        prefix = input('Type class name:')\n",
    "\n",
    "        dataset = self.path\n",
    "        name = prefix\n",
    "\n",
    "        for fname in os.listdir(dataset):\n",
    "            backup_list.append(fname)\n",
    "\n",
    "        i = 0\n",
    "        for fname in os.listdir(dataset):\n",
    "            if i < 10:\n",
    "                dst = name + '000' + str(i) + '.jpg'\n",
    "            elif (i >= 10 and i < 100):\n",
    "                dst = name + '00' +  str(i) + '.jpg'\n",
    "            else:\n",
    "                dst = name + '0' + str(i) + '.jpg'\n",
    "\n",
    "            # check to prevent overwriting existing files and if so, add a random letter suffix\n",
    "            if dst in backup_list:\n",
    "                dst = str(dst).split('.')\n",
    "                suffix = random.choice(string.ascii_lowercase)\n",
    "                dst = str(dst[0]) + str(suffix) + '.jpg'\n",
    "\n",
    "            src = dataset + fname\n",
    "            dst = dataset + dst\n",
    "            os.rename(src, dst)\n",
    "\n",
    "            i += 1\n",
    "            if i % 50 == 0:\n",
    "                print('{} files processed...'.format(i))\n",
    "\n",
    "        print('Renamed {} files\\n'.format(i))\n",
    "        print('Example: {}'.format(dst))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and run ImageDownloadProcess class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Total images to download: 50\n",
      "Specify offset: 1\n"
     ]
    }
   ],
   "source": [
    "dlp = ImageDownloadProcess(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlp.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
